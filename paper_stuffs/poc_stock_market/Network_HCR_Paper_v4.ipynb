{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Neural network Modeling</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T18:27:09.203837Z",
     "start_time": "2025-09-05T18:27:09.199404Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\X\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#from torchviz import make_dot\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "#from torchsummary import summary\n",
    "from IPython.display import Image\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import Image, display\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import ray.cloudpickle as pickle\n",
    "import tempfile\n",
    "from functools import partial\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "import gc, random, optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import HyperbandPruner\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import inspect\n",
    "from hcr_nn.models import HCRCond2D, build_hcr_cond2d\n",
    "from hcr_nn.basis import select_basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T18:27:09.222863Z",
     "start_time": "2025-09-05T18:27:09.218360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hcr_nn imported. Repo root = c:\\Users\\X\\Desktop\\basically all\\HCRNN\\HCR-NN-Library\\paper_stuffs\\poc_stock_market\n"
     ]
    }
   ],
   "source": [
    "# --- Imports & path setup -------------------------------------------------\n",
    "import os, sys, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "\n",
    "np.random.seed(42); random.seed(42); torch.manual_seed(42)\n",
    "\n",
    "# Make sure we can import hcr_nn whether we run from repo root or examples/\n",
    "CWD = os.path.abspath(os.getcwd())\n",
    "REPO_ROOT = os.path.abspath(os.path.join(CWD, '..')) if os.path.basename(CWD) == 'examples' else CWD\n",
    "if REPO_ROOT not in sys.path:\n",
    "    sys.path.insert(0, REPO_ROOT)\n",
    "\n",
    "sys.path.insert(1, '/path/to/application/app/folder')\n",
    "\n",
    "from hcr_nn.layers import CDFNorm\n",
    "from hcr_nn.models import build_hcr_cond2d, HCRCond2D\n",
    "from hcr_nn.basis import select_basis\n",
    "from hcr_nn.density import conditional_density, expected_u1_given_u2\n",
    "from hcr_nn.neuron import HCRNeuron\n",
    "\n",
    "print('hcr_nn imported. Repo root =', REPO_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T18:27:09.245416Z",
     "start_time": "2025-09-05T18:27:09.240906Z"
    }
   },
   "outputs": [],
   "source": [
    "import random, numpy as np, torch\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-05T18:27:09.260047Z",
     "start_time": "2025-09-05T18:27:09.257204Z"
    }
   },
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('../hcr_nn/layers.py'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from hcr_nn.layers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>Gathering data of stock tickers</H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\X\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL.csv', 'AXP.csv', 'BA.csv', 'CAT.csv', 'CSCO.csv', 'CVX.csv', 'DIS.csv', 'GS.csv', 'HD.csv', 'IBM.csv', 'INTC.csv', 'JNJ.csv', 'JPM1.csv', 'KO.csv', 'MCD.csv', 'MMM.csv', 'MSFT.csv', 'NKE.csv', 'PFE.csv', 'PG.csv', 'TRV.csv', 'UNH.csv', 'UTX.csv', 'V.csv', 'VZ.csv', 'WBA.csv', 'WMT.csv', 'XOM.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob, pandas as pd, os, re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "path_multiple_companies = '../paper_data/djia29'\n",
    "csv_files = glob.glob(os.path.join(path_multiple_companies, \"*.csv\"))\n",
    "\n",
    "#stuff to put before the main loop begins\n",
    "columns = ['close', 'open', 'volume', 'high', 'low']\n",
    "scores = []\n",
    "prediction_accuracies = []\n",
    "names = []\n",
    "#names.append(file.split('.')[0])\n",
    "for x in csv_files:\n",
    "    #loading dataset\n",
    "    df = pd.read_csv(x)\n",
    "\n",
    "    #changing type of column data\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['volume'] = df['volume'].astype('float64')\n",
    "    result = re.search(r'[^\\\\]+$', x).group()\n",
    "    names.append(result)\n",
    "\n",
    "\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataError",
     "evalue": "No numeric types to aggregate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\X\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:483\u001b[39m, in \u001b[36mBaseWindow._apply_series\u001b[39m\u001b[34m(self, homogeneous_func, name)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prep_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\X\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:393\u001b[39m, in \u001b[36mBaseWindow._prep_values\u001b[39m\u001b[34m(self, values)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m needs_i8_conversion(values.dtype):\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    394\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mops for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    395\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m are not implemented\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    396\u001b[39m     )\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    398\u001b[39m     \u001b[38;5;66;03m# GH #12373 : rolling functions error on float32 data\u001b[39;00m\n\u001b[32m    399\u001b[39m     \u001b[38;5;66;03m# make sure the data is coerced to float64\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: ops for ExponentialMovingWindow for this dtype datetime64[ns] are not implemented",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDataError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     31\u001b[39m     data[\u001b[33m'\u001b[39m\u001b[33mEMA\u001b[39m\u001b[33m'\u001b[39m] = data.iloc[:,\u001b[32m4\u001b[39m].ewm(com=\u001b[32m0.5\u001b[39m).mean()\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m tech_df = \u001b[43mget_tech_ind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m dataset = tech_df.iloc[\u001b[32m10\u001b[39m:,:].reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     37\u001b[39m dataset.head()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mget_tech_ind\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m     19\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mMA5\u001b[39m\u001b[33m'\u001b[39m] = data.iloc[:,\u001b[32m4\u001b[39m].rolling(window=\u001b[32m7\u001b[39m).mean() \u001b[38;5;66;03m#Close column\u001b[39;00m\n\u001b[32m     20\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mMA10\u001b[39m\u001b[33m'\u001b[39m] = data.iloc[:,\u001b[32m4\u001b[39m].rolling(window=\u001b[32m10\u001b[39m).mean() \u001b[38;5;66;03m#Close Column\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mMACD\u001b[39m\u001b[33m'\u001b[39m] = data.iloc[:,\u001b[32m4\u001b[39m].ewm(span=\u001b[32m26\u001b[39m).mean() - \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mewm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43madjust\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m#This is the difference of Closing price and Opening Price\u001b[39;00m\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Create Bands\u001b[39;00m\n\u001b[32m     26\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33m8SD\u001b[39m\u001b[33m'\u001b[39m] = data.iloc[:, \u001b[32m4\u001b[39m].rolling(\u001b[32m8\u001b[39m).std()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\X\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\window\\ewm.py:581\u001b[39m, in \u001b[36mExponentialMovingWindow.mean\u001b[39m\u001b[34m(self, numeric_only, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m     deltas = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.times \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deltas\n\u001b[32m    573\u001b[39m     window_func = partial(\n\u001b[32m    574\u001b[39m         window_aggregations.ewm,\n\u001b[32m    575\u001b[39m         com=\u001b[38;5;28mself\u001b[39m._com,\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         normalize=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    583\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mengine must be either \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnumba\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcython\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\X\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:663\u001b[39m, in \u001b[36mBaseWindow._apply\u001b[39m\u001b[34m(self, func, name, numeric_only, numba_args, **kwargs)\u001b[39m\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.method == \u001b[33m\"\u001b[39m\u001b[33msingle\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_blockwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomogeneous_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply_tablewise(homogeneous_func, name, numeric_only)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\X\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:503\u001b[39m, in \u001b[36mBaseWindow._apply_blockwise\u001b[39m\u001b[34m(self, homogeneous_func, name, numeric_only)\u001b[39m\n\u001b[32m    501\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_numeric_only(name, numeric_only)\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhomogeneous_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m obj = \u001b[38;5;28mself\u001b[39m._create_data(\u001b[38;5;28mself\u001b[39m._selected_obj, numeric_only)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == \u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    507\u001b[39m     \u001b[38;5;66;03m# GH 12541: Special case for count where we support date-like types\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\X\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\window\\rolling.py:485\u001b[39m, in \u001b[36mBaseWindow._apply_series\u001b[39m\u001b[34m(self, homogeneous_func, name)\u001b[39m\n\u001b[32m    483\u001b[39m     values = \u001b[38;5;28mself\u001b[39m._prep_values(obj._values)\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mNotImplementedError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DataError(\u001b[33m\"\u001b[39m\u001b[33mNo numeric types to aggregate\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    487\u001b[39m result = homogeneous_func(values)\n\u001b[32m    488\u001b[39m index = \u001b[38;5;28mself\u001b[39m._slice_axis_for_step(obj.index, result)\n",
      "\u001b[31mDataError\u001b[39m: No numeric types to aggregate"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "path_multiple_companies = '../paper_data/djia29'\n",
    "csv_files = glob.glob(os.path.join(path_multiple_companies, \"*.csv\"))\n",
    "companies_data = {}\n",
    "\n",
    "for x in csv_files:\n",
    "    \n",
    "    #loading dataset\n",
    "    df = pd.read_csv(x)\n",
    "\n",
    "    #changing type of column data\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['volume'] = df['volume'].astype('float64')\n",
    "\n",
    "    df5 = df.copy(deep=True)\n",
    "    df5 = df5.reset_index()\n",
    "    def get_tech_ind(data):\n",
    "        data['MA5'] = data.iloc[:,4].rolling(window=7).mean() #Close column\n",
    "        data['MA10'] = data.iloc[:,4].rolling(window=10).mean() #Close Column\n",
    "\n",
    "        data['MACD'] = data.iloc[:,4].ewm(span=26).mean() - data.iloc[:,1].ewm(span=12,adjust=False).mean()\n",
    "        #This is the difference of Closing price and Opening Price\n",
    "\n",
    "        # Create Bands\n",
    "        data['8SD'] = data.iloc[:, 4].rolling(8).std()\n",
    "        data['upper_band'] = data['MA10'] + (data['8SD'] * 2)\n",
    "        data['lower_band'] = data['MA10'] - (data['8SD'] * 2)\n",
    "\n",
    "        # Create Exponential moving average\n",
    "        data['EMA'] = data.iloc[:,4].ewm(com=0.5).mean()\n",
    "\n",
    "        return data\n",
    "\n",
    "    tech_df = get_tech_ind(df5)\n",
    "    dataset = tech_df.iloc[10:,:].reset_index(drop=True)\n",
    "    dataset.head()\n",
    "    df5['Date'] = pd.to_datetime(df5['Date'], utc=True)\n",
    "    def tech_ind(dataset):\n",
    "        fig,ax = plt.subplots(figsize=(15, 8), dpi = 200)\n",
    "        x_ = range(3, dataset.shape[0])\n",
    "        x_ = list(dataset.index)\n",
    "\n",
    "\n",
    "    tech_ind(df5)\n",
    "    dataset_5_days = df5.iloc[6:,:].reset_index(drop=True)\n",
    "    dataset_5_days.head()\n",
    "    dataset_10_days = df5.iloc[11:,:].reset_index(drop=True)\n",
    "    dataset_10_days.head()\n",
    "    dataset_5_days['Daily_Return'] = dataset_5_days['Close'].pct_change()\n",
    "\n",
    "    # jeśli nie ma trade_sign → utwórz\n",
    "    if 'trade_sign' not in dataset_5_days.columns:\n",
    "        # wariant 1: znak jednodniowej zmiany ceny Close\n",
    "        ds = dataset_5_days.copy()\n",
    "        close = ds['Close'].astype(float)\n",
    "        trade_sign = np.sign(close.diff()).fillna(0.0)  # -1, 0, 1\n",
    "        ds['trade_sign'] = trade_sign\n",
    "        dataset_5_days = ds.dropna().copy()\n",
    "\n",
    "    dataset_5_days_trade_sign = dataset_5_days[['open', 'close', 'high', 'low', 'trade_sign']]\n",
    "    # normalize 5 days data\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1)).fit(dataset_5_days_trade_sign)\n",
    "\n",
    "    data_5_days_trade_sign = scaler.transform(dataset_5_days_trade_sign)\n",
    "    seq_len_5_days=6\n",
    "    sequences_5_days_trade_sign=[]\n",
    "    for index in range(len(data_5_days_trade_sign) - seq_len_5_days + 1): \n",
    "        sequences_5_days_trade_sign.append(data_5_days_trade_sign[index: index + seq_len_5_days])\n",
    "    sequences_5_days_trade_sign = np.array(sequences_5_days_trade_sign)\n",
    "\n",
    "    sequences_5_days_trade_sign\n",
    "    valid_set_size_percentage_trade_sign = 10 \n",
    "    test_set_size_percentage_trade_sign = 10 \n",
    "\n",
    "    valid_set_size_trade_sign = int(np.round(valid_set_size_percentage_trade_sign/100*sequences_5_days_trade_sign.shape[0]))  \n",
    "    test_set_size_trade_sign  = int(np.round(test_set_size_percentage_trade_sign/100*sequences_5_days_trade_sign.shape[0]))\n",
    "    train_set_size_trade_sign = sequences_5_days_trade_sign.shape[0] - (valid_set_size_trade_sign + test_set_size_trade_sign)\n",
    "\n",
    "    x_train_5_days_trade_sign = sequences_5_days_trade_sign[:train_set_size_trade_sign,:-1,:]\n",
    "    y_train_5_days_trade_sign = sequences_5_days_trade_sign[:train_set_size_trade_sign,-1,:]\n",
    "        \n",
    "    x_valid_5_days_trade_sign = sequences_5_days_trade_sign[train_set_size_trade_sign:train_set_size_trade_sign+valid_set_size_trade_sign,:-1,:]\n",
    "    y_valid_5_days_trade_sign = sequences_5_days_trade_sign[train_set_size_trade_sign:train_set_size_trade_sign+valid_set_size_trade_sign,-1,:]\n",
    "        \n",
    "    x_test_5_days_trade_sign = sequences_5_days_trade_sign[train_set_size_trade_sign+valid_set_size_trade_sign:,:-1,:]\n",
    "    y_test_5_days_trade_sign = sequences_5_days_trade_sign[train_set_size_trade_sign+valid_set_size_trade_sign:,-1,:]\n",
    "    \n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    import torch\n",
    "\n",
    "    class SequenceDataset(Dataset):\n",
    "        \"\"\"\n",
    "        Gwarantuje spójne kształty:\n",
    "        x: (N, T-1, F)\n",
    "        y: (N, F)\n",
    "        \"\"\"\n",
    "        def __init__(self, x_np, y_np):\n",
    "            x = torch.as_tensor(x_np, dtype=torch.float32)\n",
    "            y = torch.as_tensor(y_np, dtype=torch.float32)\n",
    "\n",
    "            # Jeśli ktoś przez pomyłkę poda (N,1,F) albo (N,T,F) -> weź ostatni krok\n",
    "            if y.ndim == 3:\n",
    "                y = y[:, -1, :]\n",
    "\n",
    "            assert x.ndim == 3, f\"Expected x.ndim==3, got {x.ndim}\"\n",
    "            assert y.ndim == 2, f\"Expected y.ndim==2, got {y.ndim}\"\n",
    "            assert x.shape[0] == y.shape[0], \"x and y must have same N\"\n",
    "            assert x.shape[2] == y.shape[1], \"feature dim must match (F)\"\n",
    "\n",
    "            self.x = x\n",
    "            self.y = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.x.shape[0]\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "    def make_loaders(\n",
    "        x_train, y_train,\n",
    "        x_valid, y_valid,\n",
    "        x_test,  y_test,\n",
    "        batch_size: int = 32,\n",
    "        shuffle_train: bool = True,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "    ):\n",
    "        train_ds = SequenceDataset(x_train, y_train)\n",
    "        valid_ds = SequenceDataset(x_valid, y_valid)\n",
    "        test_ds  = SequenceDataset(x_test,  y_test)\n",
    "\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle_train,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n",
    "        valid_dl = DataLoader(valid_ds, batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n",
    "        test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
    "                            num_workers=num_workers, pin_memory=pin_memory, drop_last=False)\n",
    "        return train_dl, valid_dl, test_dl\n",
    "\n",
    "\n",
    "    # --- zbuduj dataloadery z już przygotowanych tablic ---\n",
    "    train_dataloader_5_days_trade_sign, \\\n",
    "    valid_dataloader_5_days_trade_sign, \\\n",
    "    test_dataloader_5_days_trade_sign = make_loaders(\n",
    "        x_train_5_days_trade_sign, y_train_5_days_trade_sign,\n",
    "        x_valid_5_days_trade_sign, y_valid_5_days_trade_sign,\n",
    "        x_test_5_days_trade_sign,  y_test_5_days_trade_sign,\n",
    "        batch_size=32, shuffle_train=True, num_workers=0, pin_memory=False\n",
    "    )\n",
    "\n",
    "    # Kontrola kształtów\n",
    "    xb, yb = next(iter(train_dataloader_5_days_trade_sign))\n",
    "    print(\"Batch shapes:\", tuple(xb.shape), tuple(yb.shape))  # (B, T-1, F), (B, F)\n",
    "\n",
    "    import torch.nn.functional as F  # opcjonalnie\n",
    "    from torch import nn\n",
    "\n",
    "    class NeuralNetwork(nn.Module):\n",
    "        \"\"\"\n",
    "        Wejście:  x -> (B, T, F)\n",
    "        Wyjście:  y -> (B, F)   (predykcja ostatniego kroku)\n",
    "        \"\"\"\n",
    "        def __init__(self, num_feature: int, hidden_size: int = 64, num_layers: int = 1, dropout: float = 0.0):\n",
    "            super().__init__()\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=num_feature,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout if num_layers > 1 else 0.0\n",
    "            )\n",
    "            self.head = nn.Linear(hidden_size, num_feature)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # x: (B, T, F)\n",
    "            output, (hidden, cell) = self.lstm(x)\n",
    "            last_hidden = hidden[-1]         # (B, hidden_size)\n",
    "            out = self.head(last_hidden)     # (B, F)\n",
    "            return out\n",
    "\n",
    "    model = NeuralNetwork(5)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "    #from torchinfo import summary\n",
    "    #summary(model, input_size=(32, 5, 5))  # (batch, seq_len, features)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    mse = nn.MSELoss()\n",
    "    \n",
    "    def train(dataloader, device=\"cpu\"):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(x)           # (B, F)\n",
    "            loss = mse(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        return running_loss / max(n_batches, 1)\n",
    "\n",
    "    def evaluate(dataloader, device=\"cpu\"):\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in dataloader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                pred = model(x)        # (B, F)\n",
    "                loss = mse(pred, y)\n",
    "                running_loss += loss.item()\n",
    "                n_batches += 1\n",
    "\n",
    "        return running_loss / max(n_batches, 1)\n",
    "\n",
    "    def plot_loss(train_losses, valid_losses):\n",
    "        plt.plot(train_losses, color='black', label='test loss')\n",
    "        plt.plot(valid_losses, color='blue', label='validation loss')\n",
    "\n",
    "        plt.title('Loss function comparison')\n",
    "        plt.xlabel('time')\n",
    "        plt.ylabel('loss values')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    n_epochs = 1300\n",
    "    patience = 50\n",
    "    best_valid_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch_instance in range(1, n_epochs + 1):\n",
    "        train_loss = train(train_dataloader_5_days_trade_sign, device=device)\n",
    "        valid_loss = evaluate(valid_dataloader_5_days_trade_sign, device=device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        improved = valid_loss < best_valid_loss - 1e-8\n",
    "        if improved:\n",
    "            best_valid_loss = valid_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'saved_weights.pt')  # zapisuj state_dict\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epoch_instance % 10 == 0 or improved:\n",
    "            print(f\"Epoch {epoch_instance:4d} | Train: {train_loss:.6f} | Val: {valid_loss:.6f}\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stop (patience={patience}) @ epoch {epoch_instance}\")\n",
    "            break\n",
    "\n",
    "    plot_loss(train_losses=train_losses, valid_losses=valid_losses)\n",
    "    # Ładujemy model i przełączamy w eval — UWAGA: ładujemy state_dict, a nie pełny obiekt!\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_test = NeuralNetwork(5).to(device)\n",
    "    state = torch.load('saved_weights.pt', map_location=device)\n",
    "    model_test.load_state_dict(state)\n",
    "    model_test.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_full(model, loader, device=\"cpu\"):\n",
    "        ys, ps = [], []\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            pb = model(xb)           # (B, F)\n",
    "            ys.append(yb.cpu())\n",
    "            ps.append(pb.cpu())\n",
    "        y = torch.cat(ys, dim=0).numpy()    # (N_test, F)\n",
    "        p = torch.cat(ps, dim=0).numpy()    # (N_test, F)\n",
    "        return y, p\n",
    "\n",
    "    y_test_np, y_pred_np = predict_full(model_test, test_dataloader_5_days_trade_sign, device=device)\n",
    "    print(\"Prediction shapes:\", y_test_np.shape, y_pred_np.shape)\n",
    "\n",
    "    # Rysujemy np. 'Close' = idx=1 (Open=0, Close=1, High=2, Low=3, trade_sign=4)\n",
    "    idx = 1\n",
    "    n_train = len(y_train_5_days_trade_sign)\n",
    "    x_axis = np.arange(n_train, n_train + y_test_np.shape[0])\n",
    "\n",
    "    plt.figure(figsize=(9,4))\n",
    "    plt.plot(x_axis, y_test_np[:, idx],  color='black', label='test target')\n",
    "    plt.plot(x_axis, y_pred_np[:, idx],  color='green', label='test prediction')\n",
    "    plt.title('future stock prices')\n",
    "    plt.xlabel('time [days]')\n",
    "    plt.ylabel('normalized price')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"MAE  (Close):\", mean_absolute_error(y_test_np[:, idx], y_pred_np[:, idx]))\n",
    "    print(\"MAPE (Close):\", mean_absolute_percentage_error(y_test_np[:, idx], y_pred_np[:, idx]))\n",
    "\n",
    "    ## New after Jarek sugestions\n",
    "    # === KONFIG ===\n",
    "    HORIZONS = [1, 3, 5, 10]     # w krokach (dni / interwały jak w Twoich danych)\n",
    "    INPUT_LEN = 32               # długość okna wejściowego (możesz zwiększyć)\n",
    "    TEST_SIZE = 0.15\n",
    "    VALID_SIZE = 0.15\n",
    "    BATCH_SIZE = 64\n",
    "    LR = 1e-3\n",
    "    EPOCHS = 500\n",
    "    PATIENCE = 40\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    MH_FEATURES = ['open', 'close', 'high', 'low', 'trade_sign']\n",
    "\n",
    "    # Seed dla powtarzalności\n",
    "    def set_seed(seed=42):\n",
    "        random.seed(seed); np.random.seed(seed)\n",
    "        torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    set_seed(42)\n",
    "\n",
    "    # Bierzemy dane z istniejącej ramki; jeśli nie masz 'dataset_5_days', użyj tej, którą masz po \"New\".\n",
    "    if 'dataset_5_days' in globals():\n",
    "        df_src = dataset_5_days.copy()\n",
    "    else:\n",
    "        # Jeśli w notatniku masz inną ramkę z tymi samymi kolumnami, podmień tutaj:\n",
    "        df_src = dataset_5_days_trade_sign.copy()\n",
    "\n",
    "    # Upewnij się, że kolumny są\n",
    "    missing = [c for c in MH_FEATURES if c not in df_src.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Brakuje kolumn w danych: {missing}. Podmień MH_FEATURES lub źródło danych.\")\n",
    "\n",
    "    # Skalowanie na osobnym skalerze, żeby nie popsuć wcześniejszych rzeczy\n",
    "    scaler_mh = MinMaxScaler(feature_range=(-1, 1))\n",
    "    X_full_np = scaler_mh.fit_transform(df_src[MH_FEATURES].values)   # shape: (N_all, F)\n",
    "\n",
    "    def build_multi_horizon_sequences(x_full, input_len, horizons):\n",
    "        \"\"\"\n",
    "        Zwraca:\n",
    "        X: (N, input_len, F)\n",
    "        Y: (N, H, F)  — gdzie H = len(horizons)\n",
    "        \"\"\"\n",
    "        F = x_full.shape[1]\n",
    "        H = len(horizons)\n",
    "        max_h = max(horizons)\n",
    "        seqs_X, seqs_Y = [], []\n",
    "\n",
    "        for t in range(input_len, len(x_full) - max_h):\n",
    "            x_win = x_full[t - input_len:t]                       # (input_len, F)\n",
    "            y_targets = []\n",
    "            for h in horizons:\n",
    "                y_targets.append(x_full[t + h, :])               # (F,)\n",
    "            y_targets = np.stack(y_targets, axis=0)               # (H, F)\n",
    "            seqs_X.append(x_win)\n",
    "            seqs_Y.append(y_targets)\n",
    "\n",
    "        X = np.stack(seqs_X, axis=0)   # (N, input_len, F)\n",
    "        Y = np.stack(seqs_Y, axis=0)   # (N, H, F)\n",
    "        return X, Y\n",
    "\n",
    "    X_mh, Y_mh = build_multi_horizon_sequences(X_full_np, INPUT_LEN, HORIZONS)\n",
    "\n",
    "    N = X_mh.shape[0]\n",
    "    n_test  = int(round(N * TEST_SIZE))\n",
    "    n_valid = int(round(N * VALID_SIZE))\n",
    "    n_train = N - n_test - n_valid\n",
    "    assert n_train > 0 and n_valid > 0 and n_test > 0, \"Za mało danych na taki podział.\"\n",
    "\n",
    "    X_train_mh = X_mh[:n_train]\n",
    "    Y_train_mh = Y_mh[:n_train]\n",
    "    X_valid_mh = X_mh[n_train:n_train+n_valid]\n",
    "    Y_valid_mh = Y_mh[n_train:n_train+n_valid]\n",
    "    X_test_mh  = X_mh[n_train+n_valid:]\n",
    "    Y_test_mh  = Y_mh[n_train+n_valid:]\n",
    "\n",
    "    X_train_mh.shape, Y_train_mh.shape\n",
    "\n",
    "\n",
    "    # --- Parametry HCR ---\n",
    "    FEATURE_NAME = \"Close\"\n",
    "    BASIS_NAME   = \"polynomial\"\n",
    "    DEGREE       = 16\n",
    "    GRID_SIZE    = 512\n",
    "    LR_HCR       = 1e-3\n",
    "    EPOCHS_HCR   = 800\n",
    "    PATIENCE_HCR = 134\n",
    "    EPS_DENS     = 1e-3\n",
    "\n",
    "    assert FEATURE_NAME in MH_FEATURES, f\"{FEATURE_NAME=} nie ma w MH_FEATURES={MH_FEATURES}\"\n",
    "    f_idx   = MH_FEATURES.index(FEATURE_NAME)\n",
    "    h_idx   = 0  # H=1 => indeks 0\n",
    "\n",
    "\n",
    "    def to_u(x_scaled):  # [-1,1] -> [0,1]\n",
    "        return (x_scaled + 1.0) / 2.0\n",
    "\n",
    "    u2_all = X_mh[:, -1, f_idx]\n",
    "    u1_all = Y_mh[:, h_idx, f_idx]   # target H=1\n",
    "    u2_all_u = to_u(u2_all)\n",
    "    u1_all_u = to_u(u1_all)\n",
    "\n",
    "\n",
    "    N = u2_all_u.shape[0]\n",
    "    n_test  = int(round(N * TEST_SIZE))\n",
    "    n_valid = int(round(N * VALID_SIZE))\n",
    "    n_train = N - n_test - n_valid\n",
    "\n",
    "    u2_tr, u2_va, u2_te = np.split(u2_all_u, [n_train, n_train+n_valid])\n",
    "    u1_tr, u1_va, u1_te = np.split(u1_all_u, [n_train, n_train+n_valid])\n",
    "\n",
    "\n",
    "    class HCRDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, u2, u1):\n",
    "            self.x = torch.tensor(np.stack([u1, u2], axis=1), dtype=torch.float32)\n",
    "            self.y = torch.tensor(u1, dtype=torch.float32)\n",
    "        def __len__(self): return self.x.shape[0]\n",
    "        def __getitem__(self, i): return self.x[i], self.y[i]\n",
    "\n",
    "    dl_tr = torch.utils.data.DataLoader(HCRDataset(u2_tr, u1_tr), batch_size=512, shuffle=True)\n",
    "    dl_va = torch.utils.data.DataLoader(HCRDataset(u2_va, u1_va), batch_size=1024, shuffle=False)\n",
    "    dl_te = torch.utils.data.DataLoader(HCRDataset(u2_te, u1_te), batch_size=1024, shuffle=False)\n",
    "\n",
    "    hcr = build_hcr_cond2d(\n",
    "        degree=DEGREE,\n",
    "        basis=BASIS_NAME,\n",
    "        grid_size=GRID_SIZE,\n",
    "        coeff_init=\"xavier\",\n",
    "        dtype=torch.float32,\n",
    "        device=DEVICE,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    opt = torch.optim.Adam(hcr.parameters(), lr=LR_HCR, weight_decay=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    best_va = float(\"inf\")\n",
    "    bad = 0\n",
    "    hist_hcr = {\"train\": [], \"valid\": []}\n",
    "\n",
    "    for ep in range(1, EPOCHS_HCR+1):\n",
    "        # train\n",
    "        hcr.train()\n",
    "        bl = []\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            pred = hcr(xb)         # (B,) — E[u1|u2] w [0,1]\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            bl.append(loss.item())\n",
    "        tr = float(np.mean(bl))\n",
    "\n",
    "        # valid\n",
    "        hcr.eval()\n",
    "        with torch.no_grad():\n",
    "            vl = []\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                vl.append(loss_fn(hcr(xb), yb).item())\n",
    "            va = float(np.mean(vl))\n",
    "\n",
    "        hist_hcr[\"train\"].append(tr); hist_hcr[\"valid\"].append(va)\n",
    "        improved = va < best_va - 1e-9\n",
    "        if improved:\n",
    "            best_va = va; bad = 0\n",
    "            best_state = {k: v.detach().cpu().clone() for k,v in hcr.state_dict().items()}\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        if ep % 20 == 0 or improved:\n",
    "            print(f\"[HCR {ep:4d}] train={tr:.6f} | valid={va:.6f} | best={best_va:.6f}\")\n",
    "        if bad >= PATIENCE_HCR:\n",
    "            print(f\"HCR Early stop @ {ep} (patience={PATIENCE_HCR})\")\n",
    "            break\n",
    "\n",
    "    if 'best_state' in locals():\n",
    "        hcr.load_state_dict(best_state)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_hcr(loader):\n",
    "        hcr.eval()\n",
    "        Ps, Ys = [], []\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            Ps.append(hcr(xb).cpu())\n",
    "            Ys.append(yb.cpu())\n",
    "        p = torch.cat(Ps).numpy()\n",
    "        y = torch.cat(Ys).numpy()\n",
    "\n",
    "        return 2.0*p - 1.0, 2.0*y - 1.0\n",
    "\n",
    "    u1_pred_scaled, u1_true_scaled = predict_hcr(dl_te)\n",
    "\n",
    "    def mape_safe(y_true, y_pred, eps=1e-8):\n",
    "        y_true = np.asarray(y_true).ravel()\n",
    "        y_pred = np.asarray(y_pred).ravel()\n",
    "        denom = np.maximum(np.abs(y_true), eps)\n",
    "        return float(np.mean(np.abs((y_true - y_pred)/denom)))\n",
    "\n",
    "    mae_hcr  = mean_absolute_error(u1_true_scaled, u1_pred_scaled)\n",
    "    mape_hcr = mape_safe(u1_true_scaled, u1_pred_scaled)\n",
    "    print(f\"[HCR] {FEATURE_NAME} | MAE={mae_hcr:.6f} | MAPE={mape_hcr:.6f}\")\n",
    "\n",
    "    # --- Wykres HCR vs target  ---\n",
    "    N_test_len   = u1_true_scaled.shape[0]\n",
    "    n_train_old  = len(y_train_5_days_trade_sign) if 'y_train_5_days_trade_sign' in globals() else 0\n",
    "    x_axis       = np.arange(n_train_old, n_train_old + N_test_len)\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(x_axis, u1_true_scaled,  label='Target (H=1)', lw=2.0, color='black')\n",
    "    plt.plot(x_axis, u1_pred_scaled,  label=f'HCR ({BASIS_NAME}, deg={DEGREE}, grid={GRID_SIZE})', lw=1.4)\n",
    "    plt.title(f'{FEATURE_NAME} — HCR vs target (scaled)')\n",
    "    plt.xlabel('Time index'); plt.ylabel('scaled value (-1..1)')\n",
    "    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # import optuna\n",
    "    # from sklearn.metrics import mean_absolute_error\n",
    "    #\n",
    "    # def objective(trial):\n",
    "    #\n",
    "    #     lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    #     weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-1, log=True)\n",
    "    #     patience = trial.suggest_int(\"patience\", 10, 500)\n",
    "    #\n",
    "    #     hcr = build_hcr_cond2d(\n",
    "    #         degree=DEGREE,\n",
    "    #         basis=BASIS_NAME,\n",
    "    #         grid_size=GRID_SIZE,\n",
    "    #         coeff_init=\"xavier\",\n",
    "    #         dtype=torch.float32,\n",
    "    #         device=DEVICE,\n",
    "    #     ).to(DEVICE)\n",
    "    #\n",
    "    #     opt = torch.optim.Adam(hcr.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    #     loss_fn = nn.MSELoss()\n",
    "    #\n",
    "    #     best_va = float(\"inf\")\n",
    "    #     bad = 0\n",
    "    #     for ep in range(1, EPOCHS_HCR+1):\n",
    "    #         # --- train\n",
    "    #         hcr.train()\n",
    "    #         bl = []\n",
    "    #         for xb, yb in dl_tr:\n",
    "    #             xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "    #             opt.zero_grad()\n",
    "    #             pred = hcr(xb)\n",
    "    #             loss = loss_fn(pred, yb)\n",
    "    #             loss.backward()\n",
    "    #             opt.step()\n",
    "    #             bl.append(loss.item())\n",
    "    #         tr = float(np.mean(bl))\n",
    "    #\n",
    "    #         # --- valid\n",
    "    #         hcr.eval()\n",
    "    #         with torch.no_grad():\n",
    "    #             vl = []\n",
    "    #             for xb, yb in dl_va:\n",
    "    #                 xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "    #                 vl.append(loss_fn(hcr(xb), yb).item())\n",
    "    #             va = float(np.mean(vl))\n",
    "    #\n",
    "    #         if va < best_va - 1e-9:\n",
    "    #             best_va = va; bad = 0\n",
    "    #             best_state = {k: v.detach().cpu().clone() for k,v in hcr.state_dict().items()}\n",
    "    #         else:\n",
    "    #             bad += 1\n",
    "    #\n",
    "    #         if bad >= patience:\n",
    "    #             break\n",
    "    #\n",
    "    #     if 'best_state' in locals():\n",
    "    #         hcr.load_state_dict(best_state)\n",
    "    #\n",
    "    #     @torch.no_grad()\n",
    "    #     def predict_hcr(loader):\n",
    "    #         hcr.eval()\n",
    "    #         Ps, Ys = [], []\n",
    "    #         for xb, yb in loader:\n",
    "    #             xb = xb.to(DEVICE)\n",
    "    #             Ps.append(hcr(xb).cpu())\n",
    "    #             Ys.append(yb.cpu())\n",
    "    #         p = torch.cat(Ps).numpy()\n",
    "    #         y = torch.cat(Ys).numpy()\n",
    "    #         return 2.0*p - 1.0, 2.0*y - 1.0\n",
    "    #\n",
    "    #     u1_pred_scaled, u1_true_scaled = predict_hcr(dl_te)\n",
    "    #\n",
    "    #     def mape_safe(y_true, y_pred, eps=1e-8):\n",
    "    #         y_true = np.asarray(y_true).ravel()\n",
    "    #         y_pred = np.asarray(y_pred).ravel()\n",
    "    #         denom = np.maximum(np.abs(y_true), eps)\n",
    "    #         return float(np.mean(np.abs((y_true - y_pred)/denom)))\n",
    "    #\n",
    "    #     mae_hcr  = mean_absolute_error(u1_true_scaled, u1_pred_scaled)\n",
    "    #     mape_hcr = mape_safe(u1_true_scaled, u1_pred_scaled)\n",
    "    #     print(f\"[HCR] {FEATURE_NAME} | MAE={mae_hcr:.6f} | MAPE={mape_hcr:.6f}\")\n",
    "    #\n",
    "    #     return mae_hcr\n",
    "    #\n",
    "    # study = optuna.create_study(direction='minimize')\n",
    "    # study.optimize(objective, n_trials=500)\n",
    "    # print(\"Best Hyperparameters:\", study.best_trial.params)\n",
    "\n",
    "    # --- Optuna pod HCR  ---\n",
    "\n",
    "    # ----------------------- Pomocnicze -----------------------\n",
    "\n",
    "    def set_trial_seed(base, tnum):\n",
    "        seed = int(base + tnum)\n",
    "        random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        return seed\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _predict_scaled(model, loader, device):\n",
    "        \"\"\"Zwraca (pred, true) w skali [-1,1]\"\"\"\n",
    "        model.eval()\n",
    "        Ps, Ys = [], []\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            Ps.append(model(xb).cpu())\n",
    "            Ys.append(yb.cpu())\n",
    "        p = torch.cat(Ps).numpy()\n",
    "        y = torch.cat(Ys).numpy()\n",
    "        return 2.0 * p - 1.0, 2.0 * y - 1.0\n",
    "\n",
    "\n",
    "    # ----------------------- Objective -----------------------\n",
    "\n",
    "    def objective(trial):\n",
    "        # ---- seed na trial\n",
    "        set_trial_seed(1337, trial.number)\n",
    "\n",
    "        # ---- Hiperparametry (SZERSZE zakresy/ mozna jeszcze ulepszyć)\n",
    "        degree     = trial.suggest_int(\"degree\", 2, 48, step=1)\n",
    "        grid_size  = trial.suggest_categorical(\"grid_size\", [128, 256, 512, 1024, 2048, 4096])\n",
    "        lr         = trial.suggest_float(\"lr\", 1e-6, 5e-2, log=True)\n",
    "        weight_dec = trial.suggest_float(\"weight_decay\", 1e-8, 1e-1, log=True)\n",
    "        patience   = trial.suggest_int(\"patience\", 500, min(1000, EPOCHS_HCR))\n",
    "        clip_grad  = trial.suggest_categorical(\"clip_grad\", [0.0, 0.5, 1.0, 2.0])\n",
    "        opt_kind   = trial.suggest_categorical(\"optimizer\", [\"adam\", \"adamw\"])\n",
    "\n",
    "        # ---- Model\n",
    "        hcr = build_hcr_cond2d(\n",
    "            degree=degree,\n",
    "            basis=BASIS_NAME,\n",
    "            grid_size=grid_size,\n",
    "            coeff_init=\"xavier\",\n",
    "            dtype=torch.float32,\n",
    "            device=DEVICE,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # ---- Opt/los/scheduler\n",
    "        if opt_kind == \"adam\":\n",
    "            opt = torch.optim.Adam(hcr.parameters(), lr=lr, weight_decay=weight_dec)\n",
    "        else:\n",
    "            opt = torch.optim.AdamW(hcr.parameters(), lr=lr, weight_decay=weight_dec)\n",
    "\n",
    "        loss_fn = nn.MSELoss()  # trenowanie dalej na MSE w [0,1]\n",
    "        sched   = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            opt, mode=\"min\", factor=0.5, patience=max(3, patience // 4)\n",
    "        )\n",
    "\n",
    "        best_mae = float(\"inf\")\n",
    "        bad = 0\n",
    "        best_state = None\n",
    "        min_delta = 1e-4  # wymagamy sensownej poprawy na MAE\n",
    "\n",
    "        for ep in range(1, EPOCHS_HCR + 1):\n",
    "            # --- train\n",
    "            hcr.train()\n",
    "            for xb, yb in dl_tr:\n",
    "                xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                pred = hcr(xb)           # [0,1]\n",
    "                loss = loss_fn(pred, yb) # MSE na [0,1]\n",
    "                loss.backward()\n",
    "                if clip_grad and clip_grad > 0.0:\n",
    "                    torch.nn.utils.clip_grad_norm_(hcr.parameters(), max_norm=clip_grad)\n",
    "                opt.step()\n",
    "\n",
    "            # --- valid (jednym przebiegiem liczymy MAE[-1,1])\n",
    "            p_va, y_va = _predict_scaled(hcr, dl_va, DEVICE)\n",
    "            va_mae = mean_absolute_error(y_va.ravel(), p_va.ravel())\n",
    "\n",
    "            # raport do Optuny + pruning\n",
    "            trial.report(va_mae, step=ep)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "            # scheduler + early stopping po MAE\n",
    "            sched.step(va_mae)\n",
    "            improved = (best_mae - va_mae) > min_delta\n",
    "            if improved:\n",
    "                best_mae = va_mae\n",
    "                bad = 0\n",
    "                best_state = {k: v.detach().cpu().clone() for k, v in hcr.state_dict().items()}\n",
    "            else:\n",
    "                bad += 1\n",
    "                if bad >= patience:\n",
    "                    break\n",
    "\n",
    "            # skromny log co 25 epok\n",
    "            if ep % 25 == 0 or improved:\n",
    "                print(f\"[trial {trial.number:03d} | ep {ep:4d}] val_mae={va_mae:.6f} | best={best_mae:.6f}\")\n",
    "\n",
    "        if best_state is not None:\n",
    "            hcr.load_state_dict(best_state)\n",
    "\n",
    "        # końcowa metryka walidacyjna (MAE) – to minimalizuje Optuna\n",
    "        final_p_va, final_y_va = _predict_scaled(hcr, dl_va, DEVICE)\n",
    "        mae_va = mean_absolute_error(final_y_va.ravel(), final_p_va.ravel())\n",
    "\n",
    "        # sprzątanie\n",
    "        del hcr, opt, sched\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        return mae_va\n",
    "\n",
    "\n",
    "    # ----------------------- Uruchomienie strojenia -----------------------\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"minimize\",\n",
    "        sampler=TPESampler(\n",
    "            seed=42, multivariate=True, group=True,\n",
    "            n_startup_trials=80  # więcej rozgrzewki przy szerszych zakresach\n",
    "        ),\n",
    "        pruner=HyperbandPruner(\n",
    "            min_resource=10,                # min epok zanim porównamy\n",
    "            max_resource=EPOCHS_HCR,        # pełen budżet epok\n",
    "            reduction_factor=3\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "    study.optimize(objective, n_trials=3000, show_progress_bar=True, gc_after_trial=True)\n",
    "\n",
    "    print(\"Best trial value (val MAE):\", study.best_value)\n",
    "    print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "\n",
    "\n",
    "    # ==============================\n",
    "    # Finalny model na najlepszych HP + TEST\n",
    "    # ==============================\n",
    "\n",
    "    best = study.best_trial.params\n",
    "    DEGREE_BEST    = best.get(\"degree\", 8)\n",
    "    GRID_SIZE_BEST = best.get(\"grid_size\", 256)\n",
    "    LR_BEST        = best.get(\"lr\", 1e-3)\n",
    "    WD_BEST        = best.get(\"weight_decay\", 1e-3)\n",
    "    PATIENCE_BEST  = best.get(\"patience\", 60)\n",
    "    CLIP_BEST      = best.get(\"clip_grad\", 1.0)\n",
    "    OPT_KIND_BEST  = best.get(\"optimizer\", \"adamw\")\n",
    "\n",
    "    hcr = build_hcr_cond2d(\n",
    "        degree=DEGREE_BEST,\n",
    "        basis=BASIS_NAME,\n",
    "        grid_size=GRID_SIZE_BEST,\n",
    "        coeff_init=\"xavier\",\n",
    "        dtype=torch.float32,\n",
    "        device=DEVICE,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    if OPT_KIND_BEST == \"adam\":\n",
    "        opt = torch.optim.Adam(hcr.parameters(), lr=LR_BEST, weight_decay=WD_BEST)\n",
    "    else:\n",
    "        opt = torch.optim.AdamW(hcr.parameters(), lr=LR_BEST, weight_decay=WD_BEST)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    sched   = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode=\"min\", factor=0.5, patience=max(3, PATIENCE_BEST // 4))\n",
    "\n",
    "    best_mae = float(\"inf\"); bad = 0; best_state = None\n",
    "    min_delta = 1e-4\n",
    "\n",
    "    for ep in range(1, EPOCHS_HCR + 1):\n",
    "        # train\n",
    "        hcr.train()\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            pred = hcr(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            if CLIP_BEST and CLIP_BEST > 0.0:\n",
    "                torch.nn.utils.clip_grad_norm_(hcr.parameters(), max_norm=CLIP_BEST)\n",
    "            opt.step()\n",
    "\n",
    "        # valid — MAE[-1,1]\n",
    "        p_va, y_va = _predict_scaled(hcr, dl_va, DEVICE)\n",
    "        va_mae = mean_absolute_error(y_va.ravel(), p_va.ravel())\n",
    "        sched.step(va_mae)\n",
    "\n",
    "        improved = (best_mae - va_mae) > min_delta\n",
    "        if improved:\n",
    "            best_mae = va_mae; bad = 0\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in hcr.state_dict().items()}\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        if ep % 20 == 0 or improved:\n",
    "            print(f\"[HCR {ep:4d}] val_mae_scaled={va_mae:.6f} | best={best_mae:.6f}\")\n",
    "        if bad >= PATIENCE_BEST:\n",
    "            print(f\"HCR Early stop @ {ep} (patience={PATIENCE_BEST})\")\n",
    "            break\n",
    "\n",
    "    if best_state is not None:\n",
    "        hcr.load_state_dict(best_state)\n",
    "\n",
    "    # --- TEST\n",
    "    u1_pred_scaled, u1_true_scaled = _predict_scaled(hcr, dl_te, DEVICE)\n",
    "\n",
    "    def mape_safe(y_true, y_pred, eps=1e-8):\n",
    "        y_true = np.asarray(y_true).ravel()\n",
    "        y_pred = np.asarray(y_pred).ravel()\n",
    "        denom = np.maximum(np.abs(y_true), eps)\n",
    "        return float(np.mean(np.abs((y_true - y_pred)/denom)))\n",
    "\n",
    "    mae_hcr  = mean_absolute_error(u1_true_scaled, u1_pred_scaled)\n",
    "    mape_hcr = mape_safe(u1_true_scaled, u1_pred_scaled)\n",
    "    print(f\"[HCR] {FEATURE_NAME} | TEST MAE={mae_hcr:.6f} | TEST MAPE={mape_hcr:.6f}\")\n",
    "\n",
    "    # --- Wykres HCR vs target ---\n",
    "    N_test_len   = u1_true_scaled.shape[0]\n",
    "    n_train_old  = len(y_train_5_days_trade_sign) if 'y_train_5_days_trade_sign' in globals() else 0\n",
    "    x_axis       = np.arange(n_train_old, n_train_old + N_test_len)\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(x_axis, u1_true_scaled,  label='Target (H=1)', lw=2.0, color='black')\n",
    "    plt.plot(x_axis, u1_pred_scaled,  label=f'HCR ({BASIS_NAME}, deg={DEGREE_BEST}, grid={GRID_SIZE_BEST})', lw=1.4)\n",
    "    plt.title(f'{FEATURE_NAME} — HCR vs target (scaled)')\n",
    "    plt.xlabel('Time index'); plt.ylabel('scaled value (-1..1)')\n",
    "    plt.grid(alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    class MultiHorizonDataset(Dataset):\n",
    "        \"\"\"\n",
    "        x: (N, T, F)\n",
    "        y: (N, H, F)\n",
    "        \"\"\"\n",
    "        def __init__(self, x_np, y_np):\n",
    "            x = torch.as_tensor(x_np, dtype=torch.float32)\n",
    "            y = torch.as_tensor(y_np, dtype=torch.float32)\n",
    "            assert x.ndim == 3 and y.ndim == 3, \"Oczekuję x:(N,T,F), y:(N,H,F)\"\n",
    "            assert x.shape[0] == y.shape[0] and x.shape[2] == y.shape[2], \"N i F muszą się zgadzać\"\n",
    "            self.x, self.y = x, y\n",
    "\n",
    "        def __len__(self):\n",
    "            return self.x.shape[0]\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.x[idx], self.y[idx]\n",
    "\n",
    "    def make_loaders_mh(Xtr, Ytr, Xva, Yva, Xte, Yte, bs=BATCH_SIZE, num_workers=0, pin_memory=False):\n",
    "        tr_ds = MultiHorizonDataset(Xtr, Ytr)\n",
    "        va_ds = MultiHorizonDataset(Xva, Yva)\n",
    "        te_ds = MultiHorizonDataset(Xte, Yte)\n",
    "        tr_dl = DataLoader(tr_ds, batch_size=bs, shuffle=True,  num_workers=num_workers, pin_memory=pin_memory)\n",
    "        va_dl = DataLoader(va_ds, batch_size=bs, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        te_dl = DataLoader(te_ds, batch_size=bs, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "        return tr_dl, va_dl, te_dl\n",
    "\n",
    "    train_dl_mh, valid_dl_mh, test_dl_mh = make_loaders_mh(X_train_mh, Y_train_mh, X_valid_mh, Y_valid_mh, X_test_mh, Y_test_mh)\n",
    "\n",
    "    xb, yb = next(iter(train_dl_mh))\n",
    "    print(\"Batch shapes:\", tuple(xb.shape), tuple(yb.shape))  # (B, T, F), (B, H, F)\n",
    "\n",
    "\n",
    "    F = len(MH_FEATURES)\n",
    "    H = len(HORIZONS)\n",
    "\n",
    "    class LSTM_MH(nn.Module):\n",
    "        \"\"\"\n",
    "        LSTM -> linear -> (B, H, F)\n",
    "        \"\"\"\n",
    "        def __init__(self, in_features=F, hidden_size=128, num_layers=1, dropout=0.0, horizons=H):\n",
    "            super().__init__()\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=in_features,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout if num_layers > 1 else 0.0\n",
    "            )\n",
    "            self.head = nn.Linear(hidden_size, horizons * in_features)\n",
    "            self.horizons = horizons\n",
    "            self.in_features = in_features\n",
    "\n",
    "        def forward(self, x):  # x: (B, T, F)\n",
    "            _, (h, _) = self.lstm(x)         # h: (num_layers, B, hidden)\n",
    "            h_last = h[-1]                    # (B, hidden)\n",
    "            out = self.head(h_last)           # (B, H*F)\n",
    "            out = out.view(-1, self.horizons, self.in_features)  # (B, H, F)\n",
    "            return out\n",
    "\n",
    "\n",
    "    class AdaptiveStudent(nn.Module):\n",
    "        \"\"\"\n",
    "        'Adaptacyjna EMA' z parametrami ucznia wyznaczanymi z okna wejściowego.\n",
    "        - Uczy się `alpha` w (0,1) per FEATURE (i opcjonalnie zależne od kontekstu).\n",
    "        - Generuje H prognoz rekurencyjnie: y_{t+h} = alpha * y_{t+h-1} + (1-alpha) * x_{t}\n",
    "        gdzie x_t to wektor na końcu okna wejściowego (ostatni punkt).\n",
    "        \"\"\"\n",
    "        def __init__(self, in_features=F, horizons=H, context_hidden=64):\n",
    "            super().__init__()\n",
    "            self.in_features = in_features\n",
    "            self.horizons = horizons\n",
    "            # mała sieć, która z ostatniego okna robi wektor alf\n",
    "            self.phi = nn.Sequential(\n",
    "                nn.Flatten(),                       # (B, T*F)\n",
    "                nn.Linear(INPUT_LEN*in_features, context_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(context_hidden, in_features),\n",
    "                nn.Sigmoid()                        # alpha w (0,1)\n",
    "            )\n",
    "            # waga miksu: czy używać bardziej \"ostatniej obserwacji\" czy \"poprzedniej prognozy\"\n",
    "            # Nie jest konieczna, ale pomaga stabilizować.\n",
    "            self.mix_gate = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(INPUT_LEN*in_features, in_features),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x):  # x: (B, T, F)\n",
    "            B = x.size(0)\n",
    "            last = x[:, -1, :]                   # (B, F) ostatni punkt z okna\n",
    "            alpha = self.phi(x)                  # (B, F) adaptacyjne alfy\n",
    "            gate  = self.mix_gate(x)             # (B, F) bramka miksująca\n",
    "\n",
    "            preds = []\n",
    "            y_prev = last\n",
    "            for _ in range(self.horizons):\n",
    "                # adaptacyjna ema: y_next = alpha * y_prev + (1-alpha) * last\n",
    "                y_next = alpha * y_prev + (1 - alpha) * last\n",
    "                # lekki mix z wejściem (można pominąć)\n",
    "                y_next = gate * y_next + (1 - gate) * last\n",
    "                preds.append(y_next)\n",
    "                y_prev = y_next.detach()  # odpięcie BPTT w głąb horyzontu (stabilniej); usuń .detach() jeśli chcesz pełny BPTT\n",
    "            y = torch.stack(preds, dim=1)         # (B, H, F)\n",
    "            return y\n",
    "\n",
    "    def mse_mh(pred, target):\n",
    "        # pred, target: (B, H, F)\n",
    "        return ((pred - target)**2).mean()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_mh(model, loader):\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            pb = model(xb)\n",
    "            losses.append(mse_mh(pb, yb).item())\n",
    "        return float(np.mean(losses))\n",
    "\n",
    "    def train_mh(model, train_loader, valid_loader, epochs=EPOCHS, lr=LR, patience=PATIENCE, verbose_every=10, save_path=None):\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        best_val = float('inf')\n",
    "        bad = 0\n",
    "        history = {'train': [], 'valid': []}\n",
    "\n",
    "        model.to(DEVICE)\n",
    "\n",
    "        for ep in range(1, epochs+1):\n",
    "            model.train()\n",
    "            losses = []\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                opt.zero_grad()\n",
    "                pb = model(xb)\n",
    "                loss = mse_mh(pb, yb)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                losses.append(loss.item())\n",
    "            tr = float(np.mean(losses))\n",
    "            va = evaluate_mh(model, valid_loader)\n",
    "            history['train'].append(tr); history['valid'].append(va)\n",
    "\n",
    "            improved = va < best_val - 1e-9\n",
    "            if improved:\n",
    "                best_val = va\n",
    "                bad = 0\n",
    "                if save_path:\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "            else:\n",
    "                bad += 1\n",
    "\n",
    "            if ep % verbose_every == 0 or improved:\n",
    "                print(f\"[{ep:4d}] train={tr:.6f} | valid={va:.6f} | best={best_val:.6f}\")\n",
    "\n",
    "            if bad >= patience:\n",
    "                print(f\"Early stop (patience={patience}) @ epoch {ep}\")\n",
    "                break\n",
    "\n",
    "        return history, best_val\n",
    "\n",
    "    lstm_mh = LSTM_MH(in_features=F, hidden_size=128, num_layers=1, dropout=0.0, horizons=H).to(DEVICE)\n",
    "    student  = AdaptiveStudent(in_features=F, horizons=H, context_hidden=64).to(DEVICE)\n",
    "\n",
    "    hist_student, best_student = train_mh(\n",
    "        student, train_dl_mh, valid_dl_mh,\n",
    "        epochs=EPOCHS, lr=LR, patience=PATIENCE,\n",
    "        verbose_every=10, save_path=\"student_mh.pt\"\n",
    "    )\n",
    "\n",
    "    hist_lstm, best_lstm = train_mh(\n",
    "        lstm_mh, train_dl_mh, valid_dl_mh,\n",
    "        epochs=EPOCHS, lr=LR, patience=PATIENCE,\n",
    "        verbose_every=10, save_path=\"lstm_mh.pt\"\n",
    "    )\n",
    "\n",
    "    print(\"Best valid: student:\", best_student, \" | lstm:\", best_lstm)\n",
    "\n",
    "    def ema_forecast_batch(last_window, alpha, horizons=HORIZONS):\n",
    "        \"\"\"\n",
    "        last_window: (B, T, F)\n",
    "        alpha: float w (0,1)\n",
    "        Zwraca: (B, H, F)\n",
    "        \"\"\"\n",
    "        last = last_window[:, -1, :]        # (B, F)\n",
    "        y_prev = last\n",
    "        preds = []\n",
    "        for _ in horizons:\n",
    "            y_next = alpha * y_prev + (1 - alpha) * last\n",
    "            preds.append(y_next)\n",
    "            y_prev = y_next\n",
    "        return torch.stack(preds, dim=1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_ema(loader, alpha=0.8):\n",
    "        losses = []\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            pb = ema_forecast_batch(xb, alpha, HORIZONS)\n",
    "            losses.append(mse_mh(pb, yb).item())\n",
    "        return float(np.mean(losses))\n",
    "\n",
    "    ema_val_loss = evaluate_ema(valid_dl_mh, alpha=0.8)\n",
    "    ema_test_loss = evaluate_ema(test_dl_mh, alpha=0.8)\n",
    "    print(\"EMA baseline | valid:\", ema_val_loss, \"| test:\", ema_test_loss)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_mh(model, loader):\n",
    "        model.eval()\n",
    "        Ys, Ps = [], []\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            pb = model(xb)              # (B, H, F)\n",
    "            Ps.append(pb.cpu())\n",
    "            Ys.append(yb.cpu())\n",
    "        Y = torch.cat(Ys, dim=0).numpy()\n",
    "        P = torch.cat(Ps, dim=0).numpy()\n",
    "        return Y, P\n",
    "\n",
    "    # Załaduj najlepsze wagi i licz metryki\n",
    "    lstm_mh.load_state_dict(torch.load(\"lstm_mh.pt\", map_location=DEVICE))\n",
    "    student.load_state_dict(torch.load(\"student_mh.pt\", map_location=DEVICE))\n",
    "\n",
    "    Y_test_np = Y_test_mh\n",
    "    Y_stud_np = predict_mh(student, test_dl_mh)[1]\n",
    "    Y_lstm_np = predict_mh(lstm_mh, test_dl_mh)[1]\n",
    "\n",
    "    # EMA\n",
    "    @torch.no_grad()\n",
    "    def predict_ema(loader, alpha=0.8):\n",
    "        Ps = []\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            pb = ema_forecast_batch(xb, alpha, HORIZONS)\n",
    "            Ps.append(pb.cpu())\n",
    "        P = torch.cat(Ps, dim=0).numpy()\n",
    "        return P\n",
    "\n",
    "    Y_ema_np = predict_ema(test_dl_mh, alpha=0.8)\n",
    "\n",
    "    def mse_per_horizon(pred, target):   # (N, H, F)\n",
    "        return ((pred - target)**2).mean(axis=(0,2))  # (H,) średnia po N i F\n",
    "\n",
    "    def mae_per_horizon(pred, target):\n",
    "        return (np.abs(pred - target)).mean(axis=(0,2))\n",
    "\n",
    "    print(\"MSE per H (student):\", mse_per_horizon(Y_stud_np, Y_test_np))\n",
    "    print(\"MSE per H (lstm)   :\", mse_per_horizon(Y_lstm_np, Y_test_np))\n",
    "    print(\"MSE per H (ema)    :\", mse_per_horizon(Y_ema_np,  Y_test_np))\n",
    "\n",
    "    FEATURE_TO_PLOT = 'Close'\n",
    "    f_idx = MH_FEATURES.index(FEATURE_TO_PLOT)\n",
    "\n",
    "    def dist_stats(pred_mh, target_mh, f_idx):\n",
    "        \"\"\"\n",
    "        pred_mh: (N, H, F), target_mh: (N, H, F)\n",
    "        Zwraca ramkę: średnie z H, std po H, oraz kilka kwantyli po H (po N uśredniamy)\n",
    "        \"\"\"\n",
    "        # statystyki liczymy po horyzontach (oś=1) i po N uśredniamy\n",
    "        mean_pred = pred_mh[:,:,f_idx].mean(axis=1)            # (N,)\n",
    "        std_pred  = pred_mh[:,:,f_idx].std(axis=1)             # (N,)\n",
    "        q10 = np.quantile(pred_mh[:,:,f_idx], 0.10, axis=1)    # (N,)\n",
    "        q50 = np.quantile(pred_mh[:,:,f_idx], 0.50, axis=1)\n",
    "        q90 = np.quantile(pred_mh[:,:,f_idx], 0.90, axis=1)\n",
    "        # target „następny” możemy wziąć np. h=1 (pierwszy horyzont) — to typowa referencja\n",
    "        tgt = target_mh[:,0,f_idx]                              # (N,)\n",
    "        df = pd.DataFrame({\n",
    "            \"target_h1\": tgt,\n",
    "            \"pred_mean\": mean_pred,\n",
    "            \"pred_std\": std_pred,\n",
    "            \"q10\": q10,\n",
    "            \"q50\": q50,\n",
    "            \"q90\": q90\n",
    "        })\n",
    "        return df\n",
    "\n",
    "    df_student = dist_stats(Y_stud_np, Y_test_np, f_idx)\n",
    "    df_lstm    = dist_stats(Y_lstm_np,  Y_test_np, f_idx)\n",
    "    df_ema     = dist_stats(Y_ema_np,   Y_test_np, f_idx)\n",
    "\n",
    "    def rmse(a, b): return float(np.sqrt(np.mean((a - b)**2)))\n",
    "    print(\"RMSE(mean) student:\", rmse(df_student['pred_mean'].values, df_student['target_h1'].values))\n",
    "    print(\"RMSE(mean) lstm   :\", rmse(df_lstm['pred_mean'].values,    df_lstm['target_h1'].values))\n",
    "    print(\"RMSE(mean) ema    :\", rmse(df_ema['pred_mean'].values,     df_ema['target_h1'].values))\n",
    "\n",
    "    # Wykres: porównanie „próbki rozkładu” dla jednego przykładu z testu\n",
    "    idx = np.random.randint(0, Y_test_np.shape[0])  # wybierz losową próbkę\n",
    "    x_axis = np.arange(len(HORIZONS))\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(x_axis, Y_test_np[idx,:,f_idx], label=\"Target (H)\")\n",
    "    plt.plot(x_axis, Y_stud_np[idx,:,f_idx], label=\"Student\")\n",
    "    plt.plot(x_axis, Y_lstm_np[idx,:,f_idx], label=\"LSTM\")\n",
    "    plt.plot(x_axis, Y_ema_np[idx,:,f_idx],  label=\"EMA\")\n",
    "    plt.xlabel(\"Horyzont indeks\")\n",
    "    plt.ylabel(f\"{FEATURE_TO_PLOT} (scaled)\")\n",
    "    plt.title(f\"Porównanie profilu prognoz po horyzontach (próbka {idx})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Wykres: „rozkład” (histogram) KILKU próbek po H dla danego modelu\n",
    "    def plot_empirical_distribution(P, label):\n",
    "        vals = P[:, :, f_idx].reshape(-1)    # zbierz wszystkie horyzonty i próbki\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.hist(vals, bins=40, alpha=0.8, density=True)\n",
    "        plt.title(f\"Empiryczny rozkład prognoz ({label}) — {FEATURE_TO_PLOT}\")\n",
    "        plt.xlabel(\"wartość (scaled)\"); plt.ylabel(\"gęstość\")\n",
    "        plt.show()\n",
    "\n",
    "    plot_empirical_distribution(Y_stud_np, \"Adaptive Student\")\n",
    "    plot_empirical_distribution(Y_lstm_np, \"LSTM\")\n",
    "    plot_empirical_distribution(Y_ema_np,  \"EMA\")\n",
    "\n",
    "    def mse_per_hf(pred, target):  # (N,H,F)\n",
    "        # MSE osobno dla par (h, f)\n",
    "        return ((pred - target)**2).mean(axis=0)  # (H, F)\n",
    "\n",
    "    mse_student_hf = mse_per_hf(Y_stud_np, Y_test_np)\n",
    "    mse_lstm_hf    = mse_per_hf(Y_lstm_np,  Y_test_np)\n",
    "    mse_ema_hf     = mse_per_hf(Y_ema_np,   Y_test_np)\n",
    "\n",
    "    df_mse_student = pd.DataFrame(mse_student_hf, columns=MH_FEATURES, index=[f\"H{h}\" for h in HORIZONS])\n",
    "    df_mse_lstm    = pd.DataFrame(mse_lstm_hf,    columns=MH_FEATURES, index=[f\"H{h}\" for h in HORIZONS])\n",
    "    df_mse_ema     = pd.DataFrame(mse_ema_hf,     columns=MH_FEATURES, index=[f\"H{h}\" for h in HORIZONS])\n",
    "\n",
    "    print(\"MSE per (H,F) — Student\")\n",
    "    display(df_mse_student.style.format(\"{:.6f}\"))\n",
    "\n",
    "    print(\"MSE per (H,F) — LSTM\")\n",
    "    display(df_mse_lstm.style.format(\"{:.6f}\"))\n",
    "\n",
    "    print(\"MSE per (H,F) — EMA\")\n",
    "    display(df_mse_ema.style.format(\"{:.6f}\"))\n",
    "\n",
    "    def pick_horizon(Y, h_index: int):\n",
    "        \"\"\"\n",
    "        Y: (N, H, F), zwraca (N, F) dla wybranego horyzontu.\n",
    "        h_index = 0 oznacza pierwszy krok naprzód.\n",
    "        \"\"\"\n",
    "        return Y[:, h_index, :]\n",
    "\n",
    "    def inverse_feature(arr_2d, feature_index, scaler):\n",
    "        \"\"\"\n",
    "        arr_2d: (N, F) w skali scaler_mh (-1,1)\n",
    "        Zwraca wektor (N,) dla jednej cechy w skali oryginalnej.\n",
    "        \"\"\"\n",
    "        N, F = arr_2d.shape\n",
    "        tmp = np.zeros((N, F))\n",
    "        tmp[:] = scaler.min_ / (1 - (-1)) * 0  # placeholder; i tak nadpiszemy cechę\n",
    "        tmp[:, feature_index] = arr_2d[:, feature_index]\n",
    "        inv = scaler.inverse_transform(tmp)\n",
    "        return inv[:, feature_index]\n",
    "\n",
    "    def mae_mape(pred, true, eps=1e-8):\n",
    "        \"\"\"\n",
    "        MAE i MAPE z ochroną przed dzieleniem przez 0 (MAPE).\n",
    "        pred, true: (N,) lub (N,F) (wtedy liczona po wszystkich elementach).\n",
    "        \"\"\"\n",
    "        pred = np.asarray(pred); true = np.asarray(true)\n",
    "        mae = mean_absolute_error(true.ravel(), pred.ravel())\n",
    "        # MAPE: ochronimy się przed 0 dodając eps do mianownika\n",
    "        denom = np.maximum(np.abs(true.ravel()), eps)\n",
    "        mape = np.mean(np.abs((true.ravel() - pred.ravel()) / denom))\n",
    "        return mae, mape\n",
    "\n",
    "    ## Comparision new methods\n",
    "\n",
    "    # Rysujemy np. 'Close' = idx=1 (Open=0, Close=1, High=2, Low=3, trade_sign=4)\n",
    "    feature_name = 'close'\n",
    "    idx = MH_FEATURES.index(feature_name)\n",
    "\n",
    "    # Wybieramy pierwszy horyzont (H=1 => h_index=0)\n",
    "    y_test_h1   = pick_horizon(Y_test_np, 0)       # (N, F)\n",
    "    y_stud_h1   = pick_horizon(Y_stud_np, 0)\n",
    "    y_lstm_h1   = pick_horizon(Y_lstm_np, 0)\n",
    "    y_ema_h1    = pick_horizon(Y_ema_np,  0)\n",
    "\n",
    "    # Oś czasu (doklejony do końca trainu, jak w Twoim przykładzie):\n",
    "    n_train_old = len(y_train_5_days_trade_sign) if 'y_train_5_days_trade_sign' in globals() else 0\n",
    "    x_axis = np.arange(n_train_old, n_train_old + y_test_h1.shape[0])\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(x_axis, y_test_h1[:, idx],  label='target (H=1)', lw=1.5)\n",
    "    plt.plot(x_axis, y_stud_h1[:, idx],  label='Adaptive Student', lw=1)\n",
    "    plt.plot(x_axis, y_lstm_h1[:, idx],  label='LSTM-MH', lw=1)\n",
    "    plt.plot(x_axis, y_ema_h1[:, idx],   label='EMA baseline', lw=1)\n",
    "    plt.title(f'{feature_name} — prognoza jednego kroku (H=1) — skala znormalizowana')\n",
    "    plt.xlabel('time index'); plt.ylabel('scaled value (-1..1)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # MAE/MAPE (w skali znormalizowanej)\n",
    "    for name, pred in [('Student', y_stud_h1), ('LSTM-MH', y_lstm_h1), ('EMA', y_ema_h1)]:\n",
    "        mae, mape = mae_mape(pred[:, idx], y_test_h1[:, idx])\n",
    "        print(f\"[Scaled] {name:9s} | MAE={mae:.6f} | MAPE={mape:.6f}\")\n",
    "\n",
    "    ## Final Compariosn (yet without HCR-NN)\n",
    "    # =======================\n",
    "    # Final Comparison Figure (z HCR)\n",
    "    # =======================\n",
    "\n",
    "    # ---- Config ----\n",
    "    feature_name = 'Close'  # zmień na dowolne z MH_FEATURES\n",
    "    f_idx = MH_FEATURES.index(feature_name)\n",
    "    h_index = 0             # H=1 => index 0\n",
    "    title = f\"One-step Ahead Forecast (H=1) — {feature_name} (scaled)\"\n",
    "\n",
    "    # ---- Helpers ----\n",
    "    def pick_horizon(Y, h_index: int):\n",
    "        return Y[:, h_index, :]  # (N, F)\n",
    "\n",
    "    def mape_safe(y_true, y_pred, eps=1e-8):\n",
    "        y_true = np.asarray(y_true).ravel()\n",
    "        y_pred = np.asarray(y_pred).ravel()\n",
    "        denom = np.maximum(np.abs(y_true), eps)\n",
    "        return float(np.mean(np.abs((y_true - y_pred) / denom)))\n",
    "\n",
    "    # ---- Data: targets + predictions (scaled) ----\n",
    "    # Multi-horizon targets/preds\n",
    "    y_test_h1 = pick_horizon(Y_test_np, h_index)          # (N, F)\n",
    "    series = []  # (label, values_1d)\n",
    "\n",
    "    # Adaptive Student\n",
    "    y_stud_h1 = pick_horizon(Y_stud_np, h_index)\n",
    "    mae_s  = mean_absolute_error(y_test_h1[:, f_idx], y_stud_h1[:, f_idx])\n",
    "    mape_s = mape_safe(y_test_h1[:, f_idx], y_stud_h1[:, f_idx])\n",
    "    series.append((f\"Adaptive Student (MAE={mae_s:.4f}, MAPE={mape_s:.2%})\", y_stud_h1[:, f_idx]))\n",
    "\n",
    "    # LSTM-MH\n",
    "    y_lstm_h1 = pick_horizon(Y_lstm_np, h_index)\n",
    "    mae_l  = mean_absolute_error(y_test_h1[:, f_idx], y_lstm_h1[:, f_idx])\n",
    "    mape_l = mape_safe(y_test_h1[:, f_idx], y_lstm_h1[:, f_idx])\n",
    "    series.append((f\"LSTM-MH (MAE={mae_l:.4f}, MAPE={mape_l:.2%})\", y_lstm_h1[:, f_idx]))\n",
    "\n",
    "    # EMA baseline\n",
    "    y_ema_h1 = pick_horizon(Y_ema_np, h_index)\n",
    "    mae_e  = mean_absolute_error(y_test_h1[:, f_idx], y_ema_h1[:, f_idx])\n",
    "    mape_e = mape_safe(y_test_h1[:, f_idx], y_ema_h1[:, f_idx])\n",
    "    series.append((f\"EMA baseline (MAE={mae_e:.4f}, MAPE={mape_e:.2%})\", y_ema_h1[:, f_idx]))\n",
    "\n",
    "    have_single_step = ('y_pred_np' in globals()) and ('y_test_np' in globals()) and (y_test_np.ndim == 2)\n",
    "    if have_single_step:\n",
    "        idx_old = 1  # Close w starej konwencji\n",
    "        mae_o  = mean_absolute_error(y_test_np[:, idx_old], y_pred_np[:, idx_old])\n",
    "        mape_o = mape_safe(y_test_np[:, idx_old], y_pred_np[:, idx_old])\n",
    "        series.append((f\"Single-step LSTM (MAE={mae_o:.4f}, MAPE={mape_o:.2%})\", y_pred_np[:, idx_old]))\n",
    "\n",
    "    # ---- X-axis aligned with earlier plots ----\n",
    "    n_train_old = len(y_train_5_days_trade_sign) if 'y_train_5_days_trade_sign' in globals() else 0\n",
    "    N_test_len = y_test_h1.shape[0]\n",
    "    x_axis = np.arange(n_train_old, n_train_old + N_test_len)\n",
    "\n",
    "\n",
    "    have_hcr = ('u1_pred_scaled' in globals()) and ('u1_true_scaled' in globals())\n",
    "    if have_hcr:\n",
    "        L = min(N_test_len, len(u1_pred_scaled))\n",
    "        mae_h  = mean_absolute_error(y_test_h1[:L, f_idx], u1_pred_scaled[:L])\n",
    "        mape_h = mape_safe(y_test_h1[:L, f_idx],           u1_pred_scaled[:L])\n",
    "\n",
    "        basis_label = BASIS_NAME if 'BASIS_NAME' in globals() else 'cosine'\n",
    "        deg_label   = DEGREE if 'DEGREE' in globals() else None\n",
    "        label = f\"HCR ({basis_label}\" + (f\", deg={deg_label}\" if deg_label is not None else \"\") \\\n",
    "                + f\") (MAE={mae_h:.4f}, MAPE={mape_h:.2%})\"\n",
    "        series.append((label, u1_pred_scaled[:L]))\n",
    "    else:\n",
    "        print(\"[INFO] HCR: nie znaleziono u1_pred_scaled/u1_true_scaled. Uruchom wcześniej blok HCR (Twoja biblioteka).\")\n",
    "\n",
    "    # ---- Plot ----\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    # target (scaled)\n",
    "    plt.plot(x_axis, y_test_h1[:, f_idx], label=\"Target (H=1)\", linewidth=2.0)\n",
    "\n",
    "    # modele\n",
    "    linestyles = ['-', '--', '-.', ':']  # rotacja stylów\n",
    "    for i, (lbl, vals) in enumerate(series):\n",
    "        if len(vals) != N_test_len:\n",
    "            L = min(len(vals), N_test_len)\n",
    "            plt.plot(x_axis[:L], vals[:L], label=lbl, linestyle=linestyles[i % len(linestyles)], linewidth=1.4)\n",
    "        else:\n",
    "            plt.plot(x_axis, vals, label=lbl, linestyle=linestyles[i % len(linestyles)], linewidth=1.4)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Time index\")\n",
    "    plt.ylabel(\"Scaled value (-1..1)\")\n",
    "    plt.grid(alpha=0.25)\n",
    "    plt.legend(loc=\"best\", frameon=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---- Compact metrics table ----\n",
    "    print(\"=== Scaled MAE/MAPE on H=1 —\", feature_name, \"===\")\n",
    "    print(f\"Adaptive Student: MAE={mae_s:.6f}, MAPE={mape_s:.6f}\")\n",
    "    print(f\"LSTM-MH        : MAE={mae_l:.6f}, MAPE={mape_l:.6f}\")\n",
    "    print(f\"EMA baseline   : MAE={mae_e:.6f}, MAPE={mape_e:.6f}\")\n",
    "    if have_single_step:\n",
    "        print(f\"Single-step LSTM: MAE={mae_o:.6f}, MAPE={mape_o:.6f}\")\n",
    "    if have_hcr:\n",
    "        print(f\"HCR ({basis_label}, deg={deg_label}): MAE={mae_h:.6f}, MAPE={mape_h:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
